{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdd5764c-823d-4431-8d39-4c085c42afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import requests\n",
    "import sys\n",
    "from typing import Optional, List, Tuple\n",
    "from langchain_core.language_models import BaseChatModel\n",
    "import json\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bccf8790-593d-4b2a-a479-72f6d56a35ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf103055fe4541569de7f934d9335de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5db83008fac4b188e62225e1d78df96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d2e0e24a34e4a14a2236798c6ab1da0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/94.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9964913e144b43edb1f9f50798f6aac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d3c44c751ad48fcb3124e8e2c0c20d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/777 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13fe95567d60467a863c569559fb6f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e51d1351396d4b7d9e863a006c2fd7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33578f783093435bb37a4c09815e9635",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41c0dbfaf84f4341adb636973ebe971e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a88f6d1d2a4426c8a645763702c2bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aef5bbf9a6be45d5a4a6912874a37108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Loading Pdf and Precessing it\n",
    "pdfPath = config.pdfPath\n",
    "if pdfPath is None:\n",
    "    raise ValueError(\"pdfPath is None. Please set the  pdf path in config.py.\")\n",
    "loader = PyPDFLoader(pdfPath)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,  \n",
    "        chunk_overlap=200,\n",
    "        add_start_index=True,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
    "    )\n",
    "try:\n",
    "    langchain_docs = loader.load_and_split(text_splitter=text_splitter) #loads and slits\n",
    "    #docs = loader.load()\n",
    "    #langchain_docs = text_splitter.split_documents(docs)\n",
    "except Exception as e:\n",
    "    raise ValueError(\"An error occurred:\", e)\n",
    "\n",
    "##creating Vector DB\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embeddingModelName = \"BAAI/bge-base-en-v1.5\"\n",
    "\n",
    "embeddingModel = HuggingFaceEmbeddings(model_name=embeddingModelName)\n",
    "\n",
    "db = FAISS.from_documents(langchain_docs, embeddingModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e44bf077-c08b-42b5-98b7-ae2afc95f689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f038463912c40959ee1a2218d9c7556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/638 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6237a9e65fd460699a62f095840a631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8304dea64f5648f48a9d0251f06b196c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1ae37f49ab4257b82e2cea6dd563c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00008.safetensors:   0%|          | 0.00/1.89G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411b660ed12a42aaa2e9229e15d4534d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80fe8c9d71a040b1b857d24ce618ad77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60903b7f2ef64e6692fa2fc36c0cd8ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca69adddeb2342598224b66558e0da53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566a7adb7add464f8019bf4b8efcb7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00008.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863585444c4a4f71bd0129de845a32df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00007-of-00008.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329da5044ade43faa04c2192450786d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00008-of-00008.safetensors:   0%|          | 0.00/816M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eeed5f1d6864e0199148e8cf94d05a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365c9acfd69847ebaa8f49399fde773f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a9ab75757ed435ea1d2a9384d72beb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c463b8614994dc8865a12da30110ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7968eef5c58541b8914b67801e133b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a6a54abdc84729933d29a2164d3b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4b03632f94342f9a789e8744fe1bb1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Loading the Model to answer questions\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = 'HuggingFaceH4/zephyr-7b-beta'\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "895cec60-a307-4fec-984c-2311e741787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Creating base Model Chain\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import pipeline\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    task=\"text-generation\",\n",
    "    temperature=0.2,\n",
    "    do_sample=True,\n",
    "    repetition_penalty=1.1,\n",
    "    return_full_text=True,\n",
    "    max_new_tokens=200,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "Answer the question based on your knowledge. Use the following context to help:\n",
    "{context}\n",
    "\n",
    "</s>\n",
    "<|user|>\n",
    "{question}\n",
    "</s>\n",
    "<|assistant|>\n",
    "\n",
    " \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=prompt_template,\n",
    ")\n",
    "\n",
    "# llm_chain = prompt | llm | StrOutputParser()\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "##Creating Context Chain\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# retriever = db.as_retriever()\n",
    "# retriever = db.as_retriever(search_type=\"mmr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "331c45c0-6402-43e1-b4d9-cf40d7e6a3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7875\n",
      "Running on public URL: https://b5104aaa330b6b52c6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b5104aaa330b6b52c6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:343: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.5\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:343: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.723\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/langchain_core/vectorstores.py:343: UserWarning: No relevant docs were retrieved using the relevance score threshold 0.723\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict(type, limit, question):\n",
    "    retriever = db.as_retriever(search_type=\"similarity_score_threshold\", search_kwargs={\"score_threshold\": limit})\n",
    "    rag_chain = ({\"context\": retriever, \"question\": RunnablePassthrough()}| llm_chain)\n",
    "    if type == \"Context\":\n",
    "        ragAnswer = rag_chain.invoke(question)\n",
    "        context = ragAnswer[\"context\"]\n",
    "        ans = \"Context loaded from most to least in similarity search:\"\n",
    "        i = 1\n",
    "        for c in context:\n",
    "            content = c.page_content.replace('\\n', ' ')\n",
    "            ans += \"\\n\\n\" + f\"context {i}:\" + \"\\n\\n\" + content\n",
    "            i += 1\n",
    "        return ans\n",
    "        \n",
    "    if type == \"Base\":\n",
    "        ans = llm_chain.invoke({\"context\":\"\", \"question\": question})\n",
    "        return ans\n",
    "    else:\n",
    "        res = rag_chain.invoke(question)\n",
    "        context = res[\"context\"]\n",
    "        if len(context) == 0:\n",
    "            ans = \"Please ask questions related to the documents.....\"\n",
    "        else:\n",
    "            ans = res[\"text\"]\n",
    "        return ans \n",
    "           \n",
    "\n",
    "pred = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=[\n",
    "        gr.Radio(['Context', 'BaseModel','RAG'], value = \"Context\", label=\"Select Search Type\"),\n",
    "        gr.Slider(0.1, 1, value=0.5, label=\"Degree of Similarity\"),\n",
    "        gr.Textbox(label=\"Question\"),\n",
    "    ],\n",
    "    outputs=\"text\",\n",
    "    title=\"Retrieval Augumented Generation using zephyr-7b-beta\"\n",
    ")\n",
    "\n",
    "pred.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d9ee9dde-91a6-4c65-b51f-8ccef4e87b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from datasets import Dataset, DatasetDict\n",
    "# # generated_questions = pd.read_csv(\"datasets/db2_dataset.csv\")\n",
    "# from datasets import load_dataset\n",
    "# datapoints = load_dataset(\"Ubaidbhat/StockInvestingForDummies\", split = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ff0c909-38d2-4779-a3c2-f4d189b4c622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = []\n",
    "# i = 1\n",
    "# n = len(datapoints)\n",
    "# for datapoint in datapoints:\n",
    "#     print(\"Inference number {}/{} in progress.....\".format(i, n))\n",
    "#     if i >= 591 and i <= len(datapoints) - 10:\n",
    "#         question = datapoint[\"question\"]\n",
    "#         correctAnswer = datapoint[\"answer\"]\n",
    "#         ragAnswer = rag_chain.invoke(question)\n",
    "#         baseAnswer = llm_chain.invoke({\"context\":\"\", \"question\": question}) \n",
    "#         outputs.append(\n",
    "#                 {   \n",
    "#                     \"question\": question,\n",
    "#                     \"correctAnswer\": correctAnswer,\n",
    "#                     \"ragAnswer\": ragAnswer,\n",
    "#                     \"baseModelAnswer\":baseAnswer \n",
    "                \n",
    "#                 }\n",
    "#             )\n",
    "#         generated_questions = pd.DataFrame.from_dict(outputs)\n",
    "#         generated_questions.to_csv(\"StocksQAWithZephr1.csv\", index=False)\n",
    "#     i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ea6f27c-aae4-4125-ac7f-0f9137c94d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "\n",
    "# def predict(type, question):\n",
    "#     if type == \"Base\":\n",
    "#         ans = llm_chain.invoke({\"context\":\"\", \"question\": question})\n",
    "#         return ans\n",
    "#     else:\n",
    "#         ans = rag_chain.invoke(question)\n",
    "#         return ans    \n",
    "\n",
    "# pred = gr.Interface(\n",
    "#     fn=predict,\n",
    "#     inputs=[\n",
    "#         gr.Radio(['Base', 'Context'], label=\"Select One\"),\n",
    "#         gr.Textbox(label=\"Question\"),\n",
    "#     ],\n",
    "#     outputs=\"text\",\n",
    "#     title=\"Retrieval Augumented Generation using zephyr-7b-beta\"\n",
    "# )\n",
    "\n",
    "# pred.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
